-------------------------------------------------2018-12-27----------------------------------------------

问题：
输出文件前几行就是乱码。

解决：
考虑编码问题，将 string 读成 wstring ，然后各种用转换编码的库和网上的代码，就是不行。
（这里的不行是说代码没跑起来，不是说没解决问题。。。。）
让黄莹帮忙输出了一下，很简单，vector<string> 直接 cout 就可以了。
所以问题实际上是系统编码的问题。。。衰。。。直接调整 shell 的编码为 utf-8。
此时文件的输出对了，但是我自己 cout 的中文还有中文注释都不对了。
此时意识到是系统编码的问题。。看了一下系统的环境变量 $LANG 是 UTF-8，但是 windows 的默认编码是 GBK。
这才导致了到了 ubuntu 中，使用默认 utf8 方式显示不出来。
所以解决方式就是直接写个脚本处理编码。


-------------------------------------------------2018-12-28----------------------------------------------

问题1：
实现编码转换脚本，过程中顺便解决一个小问题就是文件名有事特别乱，什么中文完本精校版。
编码预处理过程中顺便把文件名也改过来。
使用到的bash工具：
- dirname、bashname、realpath
- iconv、file

问题2：
分词时遇到问题就是现在的分词是按行的，但是我需要拿到词对于分档的偏移。
这个对于 jieba 的 cut 函数是有相关的重载版本的，是要传入的参数是内置的 Word 结构体就行。
但是这个结构体偏移是针对一行文字的，而不是整个文档，所以我要做倒排索引需要转化为整个文档的偏移。
这个可以通过行的长度一直累加算出来，utf编码是变长编码，一个汉字在utf-8编码下通常是三个字节。
然后加上换行符就可以按行处理，计算总偏移量。这是出现另一个小问题。
就是 windows 和 linux 的默认换行符也不同，通过 file 也可以看到。
这就直接导致了换行是需要一个还是两个字节。
所以还是老样子，简单方式处理，默认处理为\n换行。
工具：
- dos2unix、tr

上面两个工具都可以一行解决这个问题，用 time 命令比较了一下二者执行时间。
一个 1.6M 的文件都说 dos2unix 总时间需要0.1s，tr需要0.05s。
内核态执行时间差不多，但是用户态 dos2unix 明显耗时长的多得多。
所以选择用tr来做。


-------------------------------------------------2018-12-29----------------------------------------------

问题1：
有的文件可能是为了手机阅读，换行不是按照一句话划分的，而是指定字数。
这就使得分词会出问题。

解决：
用了bash写了一个很粗糙的方案。


问题2：
还有的文件存在大量空行。

解决：
这个还是选择了用脚本预处理，要是写在 C++ 的 parser 类中也行。
但是这会导致 parser 传给分词器的是一句完整的句子，但是后面求词相对于文档的偏移量的话就会不好求。
所以在预处理脚本中，在另外写了一个过滤模块，过滤 \r 、多余的 \n 以及多余的空格。
因为tr是处理流的，所以这三个功能依次写成管道就可以。
第一个管道过滤\r。
过滤后会有大量的\n聚集在一起，然后过滤\n，同理过滤空格。


-------------------------------------------------2018-12-30----------------------------------------------
问题1：
C++IDE的调试问题。

解决：
解决了一天调试问题，最后也不知道问题出在了哪里。但是大概缕清楚了编译调试的流程。
可能的原因是编译时需要加上 -g 选项来加入调试信息。
然后使用 gdb 来调试编译好的可运行文件，这里网上说一定要保证路径没有中文。
但是经过我的测试，中文路径下也可以编译成功并进行调试。
所以最终也不知道为啥，但是现在是好使的就行了。然后把编译器从g++改成了clang。


-------------------------------------------------2018-12-31----------------------------------------------
问题1：
对于12-29的问题1，只有很少的文档有这种问题，那么如果自动判断需不需要进行这个处理呢，怎么弄？
因为每一个文件都这么弄代价有些大。

问题2：
目前解析的是小说的txt文件，后续还想做 markdown 文档，所以需要加一个 parser 类。

解决：
已完成 BaseParser 和 DocParser ，作为接口的虚函数是getContent。

问题3：
小说文本中有目录，分词也没啥用，大家格式也不一样。还有就是标点符号也在分词的结果中，这部分要过滤掉。
但是这个是小问题，要关注本质复杂度。


-------------------------------------------------2019-1-2----------------------------------------------
问题1：
倒排索引的基本构建已经完成，下面要斟酌一下的是 InverseIndex中的几个核心结构是否要用 shared_ptr 。
这个对于析构或者拷贝构造函数是否有影响。还有类似的问题就是 query 函数的结果返回应该是一个倒排列表。
这个返回值是用对象还是指针还是引用返回？

问题2：
完成逻辑AND的短语检索。

问题3：
cli交互这部分的代码怎么抽象出来？



-------------------------------------------------2019-1-3----------------------------------------------
问题1：
由于在进行短语查询的时候涉及到多个倒排列表的归并，所以需要给文件一个id，随着一个个扫描文件并添加进索引，可以保证倒排列表中的文件id是单调增的。
现在的问题是最好的情况是用数据库存，但是我现在想用文件存，怎么设计这个通用的接口进行持久化？

